{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f9905742",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, PReLU, Flatten, Softmax, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0ed878d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all words\n",
    "words=[]\n",
    "# list of all category of intents\n",
    "classes = []\n",
    "documents = []\n",
    "json_file_name = './intents-generated.json'\n",
    "csv_file_name = './mydata1.csv'\n",
    "data = pd.read_csv(csv_file_name)\n",
    "# data = data.sample(1000)\n",
    "myintents = open(json_file_name,encoding=\"utf-8\").read()\n",
    "intents = json.loads(myintents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f171c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to the classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a20563ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in stop_words]\n",
    "words = sorted(list(set(words)))\n",
    "# print(\"words\",words)\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "# print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "# print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "# print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "# pickle.dump(words,open('words.pkl','wb'))\n",
    "# pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "38197504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4681e241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6085/6085 [==============================] - 17s 3ms/sample - loss: 1.3033 - accuracy: 0.4460\n",
      "Epoch 2/5\n",
      "6085/6085 [==============================] - 15s 2ms/sample - loss: 0.9829 - accuracy: 0.5915\n",
      "Epoch 3/5\n",
      "6085/6085 [==============================] - 15s 2ms/sample - loss: 0.7657 - accuracy: 0.7019\n",
      "Epoch 4/5\n",
      "6085/6085 [==============================] - 15s 2ms/sample - loss: 0.6327 - accuracy: 0.7574\n",
      "Epoch 5/5\n",
      "6085/6085 [==============================] - 13s 2ms/sample - loss: 0.5191 - accuracy: 0.8131\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=10, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)\n",
    "#     print(len(words))\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag:\",w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(\"could you give me some advice for travling in France\",model)[0][\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54cbc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(\"Can I restart my computer directly\",model)[0][\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21bb6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_class(\"restart mysql service\",model)[0][\"intent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "afec694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max5(arr):\n",
    "\n",
    "    ixarr = []\n",
    "    for ix, el in enumerate(arr):\n",
    "        ixarr.append((el, ix))\n",
    "    ixarr.sort()\n",
    "    ixs = []\n",
    "    for i in ixarr[-5:]:\n",
    "        ixs.append(i[1])\n",
    "    return ixs[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21e0fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "oral = \"Bot: Hi, Nice to hear from you!\"\n",
    "def feedback(n,limit):\n",
    "    if(n==limit):\n",
    "        print(\"Sorry about that, I am not sure I understand...\")\n",
    "        return\n",
    "    else:\n",
    "        global data\n",
    "        question = questionset[\"question\"][questionset.index[inds[n+1]]]\n",
    "        answer =  data['answer'][questionset.index[inds[n+1]]]\n",
    "        print(\"===\"*90)\n",
    "        print(\"Bot: \"+data['answer'][questionset.index[inds[n+1]]])\n",
    "        print(\"===\"*90)\n",
    "        usr = input(\"Was this answer helpful? Yes/No: \").lower().strip()\n",
    "        if usr == 'yes':\n",
    "            global oral \n",
    "            oral = \"What can I help you now?\"\n",
    "            data = data.append({'question': original.strip().lower(), 'answer': answer, 'tag': pred_tag}, ignore_index=True)\n",
    "            data.to_csv(csv_file_name,index=False)\n",
    "            over = True\n",
    "            return\n",
    "        elif usr == \"no\":\n",
    "            feedback(n+1,limit)\n",
    "        else:\n",
    "            print(\".........????????\")\n",
    "            over = True\n",
    "            oral = \"Bot: Hi, Nice to hear from you!\"\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "179ed19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "def cleanup(sentence):\n",
    "    word_tok = nltk.word_tokenize(sentence)\n",
    "    stemmed_words = [stemmer.stem(w) for w in word_tok]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "le = LE()\n",
    "tfv = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "questions = data['question'].values\n",
    "X = []\n",
    "for question in questions:\n",
    "    X.append(cleanup(question))\n",
    "tfv.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694ddca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,may I have your name?\n",
      "You: My name is Ben\n",
      "Bot: Hi Ben, Nice to hear from you!\n",
      "PRESS Q to QUIT\n",
      "You: Could you give me some advice for travling in the UK\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Bot: I hope I figured it out. I have no more Black screen issue &amp; flickering since I did below  yesterday:\n",
      "\n",
      "\n",
      "Found advise from a user and Killed process called \"warmd\", process user: unknown.\n",
      "http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man8/warmd.8.html\n",
      "When my Screen went black sporadically, My ipad was connected and nearby my MacBook , I relocated it further away (I read about interference issues)\n",
      "I right clicked on battery icon and it showed me that Battery needs Service. I fully re-charged and re-booted my Pro and WoohLah, Battery status was back to normal....\n",
      "I switched GPU >> Installed programm called GFXcardstatus (you can Google that)\n",
      "In Applications->Utilities->Terminal\n",
      "enter: sudo pmset -a lidwake 0   to reverse replace 0 with 1\n",
      "enter your computer password...\n",
      "\n",
      "\n",
      "This sends your laptop to sleep on closing but on opening tap any key to wake. According to advise by other user on the net this has stopped his issue with black screen and lastly I performed it after step 1-4.\n",
      "\n",
      "Not sure what caused it tho but fiddeling around with above sorted my issue.\n",
      "\n",
      "\n",
      "\n",
      "I posted this comment in the apple discussions forum.\n",
      "\n",
      "I hope that helps.\n",
      "\n",
      "Good luck to all of you!\n",
      "\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Was this answer helpful? Yes/No: NO\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Bot: Maybe help if can be fixes origin of this error. It happened because something write to file with overwrite \">\" not with append \">>\".\n",
      "\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Was this answer helpful? Yes/No: NO\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Bot: I hope I figured it out. I have no more Black screen issue &amp; flickering since I did below  yesterday:\n",
      "\n",
      "\n",
      "Found advise from a user and Killed process called \"warmd\", process user: unknown.\n",
      "http://developer.apple.com/library/mac/#documentation/Darwin/Reference/ManPages/man8/warmd.8.html\n",
      "When my Screen went black sporadically, My ipad was connected and nearby my MacBook , I relocated it further away (I read about interference issues)\n",
      "I right clicked on battery icon and it showed me that Battery needs Service. I fully re-charged and re-booted my Pro and WoohLah, Battery status was back to normal....\n",
      "I switched GPU >> Installed programm called GFXcardstatus (you can Google that)\n",
      "In Applications->Utilities->Terminal\n",
      "enter: sudo pmset -a lidwake 0   to reverse replace 0 with 1\n",
      "enter your computer password...\n",
      "\n",
      "\n",
      "This sends your laptop to sleep on closing but on opening tap any key to wake. According to advise by other user on the net this has stopped his issue with black screen and lastly I performed it after step 1-4.\n",
      "\n",
      "Not sure what caused it tho but fiddeling around with above sorted my issue.\n",
      "\n",
      "\n",
      "\n",
      "I posted this comment in the apple discussions forum.\n",
      "\n",
      "I hope that helps.\n",
      "\n",
      "Good luck to all of you!\n",
      "\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Was this answer helpful? Yes/No: NO\n",
      "Sorry about that, I am not sure I understand...\n",
      "PRESS Q to QUIT\n",
      "You: cAN YOU PLAY MUSIC IN MY ROOM\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Bot: It's the room of requirement. Voldemort required a room to hide his item in. Where better to hide a small, reasonably nondescript item than in a pile of junk? If not then it's possible when he hid it it wasn't full of junk; Harry required a room full of all of the small, nondescript items that may have been the diadem. Yes he knew what it was but maybe not specifically enough to single it out.\n",
      "\n",
      "The room also has a habit of overdoing things, like when Dumbledore needs the loo and finds tens of chamberpots there, when one would do: \n",
      "\n",
      "\n",
      "  \"Only this morning, for instance, I took a wrong turning on the way to the bathroom and found myself in a beautifully proportioned room I have never seen before, containing a really rather magnificent collection of chamber pots. When I went back to investigate more closely, I discovered that the room had vanished. But I must keep an eye out for it. Possibly it is only accessible at five-thirty in the morning. Or it may only appear at the quarter moon - or when the seeker has an exceptionally full bladder.\" \n",
      "\n",
      "\n",
      "~Christmas Ball, Goblet of Fire,\n",
      "\n",
      "==============================================================================================================================================================================================================================================================================\n",
      "Was this answer helpful? Yes/No: YES\n",
      "PRESS Q to QUIT\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('username.pkl', 'rb') as file:  \n",
    "        username = pickle.load(file)\n",
    "        oral = \"Bot: Hi \"+username+\", Nice to hear from you!\"\n",
    "except:\n",
    "    oral = 'Hello,may I have your name?'\n",
    "    print(oral)\n",
    "    sentence_name = input(\"You: \")\n",
    "    doc = nlp(sentence_name)\n",
    "    if len(doc.ents)<=0:\n",
    "        print(\"Maybe I can only name you sweetie :p\")\n",
    "        username = \"sweetie\"\n",
    "    else:\n",
    "        for ent in doc.ents:\n",
    "            if(ent.label_ ==\"PERSON\"):\n",
    "                username = ent.text\n",
    "            else:\n",
    "                print(\"Maybe I can only name you sweetie :p\")\n",
    "                username = \"sweetie\"\n",
    "    pickle.dump(username,open('username.pkl','wb'))\n",
    "oral = \"Bot: Hi \"+username+\", Nice to hear from you!\"\n",
    "print(oral)\n",
    "while True:\n",
    "    over = False\n",
    "    print(\"PRESS Q to QUIT\")\n",
    "    original = input(\"You: \")\n",
    "    if original.strip().lower()==\"q\":\n",
    "        print(\"Bot: It was good to be of help, \",username)\n",
    "        break\n",
    "    else:\n",
    "        pred_tag = predict_class(original.strip().lower(),model)[0][\"intent\"]\n",
    "        questionset = data[data['tag'] == pred_tag]\n",
    "        cos_sims = []\n",
    "        t_original = tfv.transform([cleanup(original.strip().lower())])\n",
    "        for question in questionset['question']:\n",
    "            sims = cosine_similarity(tfv.transform([question]), t_original)\n",
    "            cos_sims.append(sims)\n",
    "        inds = get_max5(cos_sims)\n",
    "        if over==False:\n",
    "            feedback(0,3)\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6340c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48111ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}